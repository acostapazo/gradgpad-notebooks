{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The GRAD-GPAD framework üóø \n",
    "    \n",
    "    ‚û°Ô∏è Foundations: The Python Library\n",
    "---\n",
    "\n",
    "The `gradgpad` framework povide python modules to ease face-PAD research. This tutorial is designed to help you become familiar with the package, undertanding the **Foundations** of the package.\n",
    "\n",
    "\n",
    "![gradgpad_detailed_architecture](images/gradgpad_detailed_architecture.jpeg \"GRAD-GPAD Architecture\")\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents üë©‚Äçüíª\n",
    "- Installation üíª\n",
    "- Environment\n",
    "- Annotations\n",
    "- Scores\n",
    "- Results\n",
    "- Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†Installation üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U gradgpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Environment\n",
    "\n",
    "We start importing required objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gradgpad import (\n",
    "    annotations,\n",
    "    ScoresProvider,\n",
    "    Approach,\n",
    "    Protocol,\n",
    "    Subset,\n",
    "    Dataset,\n",
    "    Device,\n",
    "    CoarseGrainedPai,\n",
    "    GrainedPaiMode,\n",
    "    Demographic,\n",
    "    Filter,\n",
    "    Sex,\n",
    "    Age,\n",
    "    SkinTone,\n",
    "    Scenario,\n",
    "    Metrics,\n",
    "    MetricsDemographics,\n",
    "    ResultsProvider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###¬†Annotations\n",
    "\n",
    "There are nearly 30000 annotations for the 13 datasets presented in the GRAD-GPAD v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert annotations.num_annotations == 29567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print each of the annotations using sorted index. Note if not select an annotation index it will print the `29567` annotations, but this is not recommended to use on a Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotations.annotated_samples[0])\n",
    "annotations.print_semantic(annotation_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also, filter by dataset ids as the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ids = [\n",
    "    \"replay-mobile_devel/real/client023_session02_authenticate_tablet_controlled.mov\", \n",
    "    \"replay-mobile_devel/real/client025_session02_authenticate_tablet_controlled.mov\",\n",
    "    \"replay-mobile_devel/real/client029_session02_authenticate_mobile_direct.mov\",\n",
    "    \"replay-mobile_devel/real/client005_session02_authenticate_mobile_controlled.mov\"]\n",
    "selected_annotations = annotations.get_annotations_from_ids(selected_ids)\n",
    "assert len(selected_annotations) == len(selected_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain statistics from `annotations` with `statistics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = annotations.statistics()\n",
    "print(json.dumps(statistics, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stadistic are reflected on work [CITATION] Figure XXXX\n",
    "\n",
    "<img src=\"images/grad-gpad-pais.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Scores\n",
    "\n",
    "The `gradgpad` library has available a class to provide scores (`ResultProvider`).\n",
    "To retrieve all scores for a specific approach, use `ScoresProvider.all(approach)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_quality = ScoresProvider.all(Approach.QUALITY_RBF)\n",
    "scores_auxiliary = ScoresProvider.all(Approach.AUXILIARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve all scores by subsets, use `ScoresProvider.get_subsets(approach, protocol)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_auxiliary_subsets = ScoresProvider.get_subsets(\n",
    "    Approach.AUXILIARY, \n",
    "    Protocol.GRANDTEST\n",
    ")\n",
    "scores_auxiliary_devel = scores_auxiliary_subsets.get(\"devel\")\n",
    "scores_auxiliary_test = scores_auxiliary_subsets.get(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, you can obtain scores with `ScoresProvider.get(approach, protocol, subset, dataset, device, pai)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_auxiliary_test = ScoresProvider.get(\n",
    "    Approach.AUXILIARY, \n",
    "    Protocol.GRANDTEST, \n",
    "    Subset.TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more specific scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_specific = ScoresProvider.get(\n",
    "    Approach.AUXILIARY, \n",
    "    Protocol.GRANDTEST, \n",
    "    Subset.TEST, \n",
    "    Dataset.REPLAY_MOBILE, \n",
    "    Device.MOBILE_TABLET, \n",
    "    CoarseGrainedPai.REPLAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check every option with static method `options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- Approaches:\\n  > {Approach.options()}\\n\")\n",
    "print(f\"- Protocol:\\n  > {Protocol.options()}\\n\")\n",
    "print(f\"- Subset:\\n  > {Subset.options()}\\n\")\n",
    "print(f\"- Dataset:\\n  > {Dataset.options()}\\n\")\n",
    "print(f\"- CoarseGrainedPai:\\n  > {CoarseGrainedPai.options()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, provided `Scores` class implements useful tools to manage obtained scores from an experiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ScoresProvider.get(\n",
    "    Approach.AUXILIARY, \n",
    "    Protocol.GRANDTEST, \n",
    "    Subset.TEST\n",
    ")\n",
    "\n",
    "genuine_scores = scores.get_genuine()\n",
    "genuine_attacks = scores.get_attacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `numpy` for data management in your experiments, just use `get_numpy_scores` and `get_numpy_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_scores_genuine = scores.get_numpy_scores()\n",
    "numpy_scores_labels = scores.get_numpy_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting tool that implements `Scores` object is the filtering method `filtered_by` wich uses `Filter` object.\n",
    "\n",
    "Imagine we want filter by `Sex`, `Age` or `SkinTone`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex\n",
    "male_scores = scores.filtered_by(Filter(sex=Sex.MALE))\n",
    "female_scores = scores.filtered_by(Filter(sex=Sex.FEMALE))\n",
    "\n",
    "# Age\n",
    "young_scores = scores.filtered_by(Filter(age=Age.YOUNG))\n",
    "adult_scores = scores.filtered_by(Filter(age=Age.ADULT))\n",
    "senior_scores = scores.filtered_by(Filter(age=Age.SENIOR))\n",
    "\n",
    "# SkinTone\n",
    "light_pink_scores = scores.filtered_by(Filter(skin_tone=SkinTone.LIGHT_PINK))\n",
    "light_yellow_scores = scores.filtered_by(Filter(skin_tone=SkinTone.LIGHT_YELLOW))\n",
    "medium_pink_brown_scores = scores.filtered_by(Filter(skin_tone=SkinTone.MEDIUM_PINK_BROWN))\n",
    "medium_dark_brown_scores = scores.filtered_by(Filter(skin_tone=SkinTone.MEDIUM_DARK_BROWN))\n",
    "dark_brown_scores = scores.filtered_by(Filter(skin_tone=SkinTone.DARK_BROWN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just filter by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_fasd_scores = scores.filtered_by(Filter(dataset=Dataset.CASIA_FASD))\n",
    "threedmad_scores = scores.filtered_by(Filter(dataset=Dataset.THREEDMAD))\n",
    "uvad_scores = scores.filtered_by(Filter(dataset=Dataset.UVAD))\n",
    "siw_m_scores = scores.filtered_by(Filter(dataset=Dataset.SIW_M))\n",
    "siw_scores = scores.filtered_by(Filter(dataset=Dataset.SIW))\n",
    "rose_youtu_scores = scores.filtered_by(Filter(dataset=Dataset.ROSE_YOUTU))\n",
    "replay_mobile_scores = scores.filtered_by(Filter(dataset=Dataset.REPLAY_MOBILE))\n",
    "replay_attack_scores = scores.filtered_by(Filter(dataset=Dataset.REPLAY_ATTACK))\n",
    "oulu_npu_scores = scores.filtered_by(Filter(dataset=Dataset.OULU_NPU))\n",
    "msu_mfsd_scores = scores.filtered_by(Filter(dataset=Dataset.MSU_MFSD))\n",
    "hkbu_v2_scores = scores.filtered_by(Filter(dataset=Dataset.HKBU_V2))\n",
    "hkbu_scores = scores.filtered_by(Filter(dataset=Dataset.HKBU))\n",
    "csmad_scores = scores.filtered_by(Filter(dataset=Dataset.CSMAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by scenario:\n",
    "* **Genuine** (`Scenario.GENUINE`): Genuine attemps;\n",
    "* **PAS I** (`Scenario.PAS_TYPE_I`): represents a scenario where spoofers have freedom to try to impersonate an identity completely (as with a stolen cell phone or in an isolated access environment);\n",
    "* **PAS II** (`Scenario.PAS_TYPE_II`): represents partial identity impersonations, where attackers only use a part of someone else‚Äôs identity;\n",
    "* **PAS III** (`Scenario.PAS_TYPE_III`): where users try to hide their identity without impersonating anyone in particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_scores = scores.filtered_by(Filter(scenario=Scenario.GENUINE))\n",
    "pas_I_scores = scores.filtered_by(Filter(scenario=Scenario.PAS_TYPE_I))\n",
    "pas_II_scores = scores.filtered_by(Filter(scenario=Scenario.PAS_TYPE_II))\n",
    "pas_III_scores = scores.filtered_by(Filter(scenario=Scenario.PAS_TYPE_III))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `Scores` class provides you fair subsets (balanced) for demographic labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_fair_subset = scores.get_fair_sex_subset()\n",
    "age_fair_subset = scores.get_fair_age_subset()\n",
    "skin_tone_fair_subset = scores.get_fair_skin_tone_subset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Metrics\n",
    "\n",
    "There are three ways of calculating metrics using the `gradgpad` `Python` package: using the `Metrics`\n",
    "and `MetricsDemographic` classes and/or using the available functions on module `gradgpad.foundations.metrics`. \n",
    "\n",
    "First, if we use the `Metrics` class, we can automatize the calculation of basic Metrics (e.g *EER*): Besides, it\n",
    "provides a method to calculate an in-depth analysis taking into account fine and coarse grained PAI categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_auxiliary_devel = ScoresProvider.get(\n",
    "    Approach.AUXILIARY, \n",
    "    Protocol.GRANDTEST, \n",
    "    Subset.DEVEL\n",
    ")\n",
    "\n",
    "scores_auxiliary_test = ScoresProvider.get(\n",
    "    Approach.AUXILIARY, \n",
    "    Protocol.GRANDTEST, \n",
    "    Subset.TEST\n",
    ")\n",
    "\n",
    "metrics = Metrics(\n",
    "    devel_scores=scores_auxiliary_devel,\n",
    "    test_scores=scores_auxiliary_test\n",
    ")\n",
    "\n",
    "# Obtain Equal Error Rates values for DEVEL and TEST subsets\n",
    "eer_devel = metrics.get_eer(Subset.DEVEL)\n",
    "eer_test = metrics.get_eer(Subset.TEST)\n",
    "eer_devel_threshold = metrics.get_eer_th(Subset.DEVEL)\n",
    "eer_test_threshold = metrics.get_eer_th(Subset.TEST)\n",
    "\n",
    "#¬†Obtain an in-depth analysis from the values of DEVEL and TEST subsets \n",
    "#¬†fixing the working points for BPCER and APCER ([0-1]).\n",
    "bpcer_fixing_working_points = [\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.20\n",
    "]  \n",
    "apcer_fixing_working_points = [\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.20,\n",
    "]  \n",
    "indepth_analysis = metrics.get_indepth_analysis(\n",
    "    bpcer_fixing_working_points, \n",
    "    apcer_fixing_working_points,\n",
    "    GrainedPaiMode.options()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUXILIARY EER [Devel: {metrics.get_eer(Subset.DEVEL)} |¬†Test: {metrics.get_eer(Subset.TEST)}]\")\n",
    "print(f\"AUXILIARY EER Th [Devel: {metrics.get_eer_th(Subset.DEVEL)} |¬†Test: {metrics.get_eer_th(Subset.TEST)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(indepth_analysis, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains many detailed metrics for both fine-grained and coarse-grained PAI calculation. \n",
    "\n",
    "On the other hand, if we want to calculate specific metrics for usability on demographic groups, the package provdes the `MetricsDemographic` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_demographics = MetricsDemographics(\n",
    "    devel_scores=scores_auxiliary_devel,\n",
    "    test_scores=scores_auxiliary_test\n",
    ")\n",
    "\n",
    "bpcer_sex = metrics_demographics.get_bpcer_sex()\n",
    "bpcer_age = metrics_demographics.get_bpcer_age()\n",
    "bpcer_skin_tone = metrics_demographics.get_bpcer_skin_tone()\n",
    "\n",
    "\n",
    "print(json.dumps(bpcer_sex, indent=4, sort_keys=True))\n",
    "print(json.dumps(bpcer_age, indent=4, sort_keys=True))\n",
    "print(json.dumps(bpcer_skin_tone, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, functions provided in the module `gradgpad.fountations.metrics` can be used to calculate specific metrics as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradgpad.foundations.metrics.eer import eer\n",
    "from gradgpad.foundations.metrics.far import far\n",
    "from gradgpad.foundations.metrics.frr import frr\n",
    "\n",
    "devel_scores = scores_auxiliary_devel.get_numpy_scores()\n",
    "devel_labels = scores_auxiliary_devel.get_numpy_labels()\n",
    "\n",
    "eer_value, eer_threshold = eer(devel_scores, devel_labels)\n",
    "far_01_value, far_01_threshold = far(devel_scores, devel_labels, 0.01)\n",
    "ffr_05_value, frr_05_threshold = frr(devel_scores, devel_labels, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUXILIARY EER [Devel: {eer_value*100:.2f} % (threshold {eer_threshold:.2f})]\")\n",
    "print(f\"AUXILIARY FAR @¬†FRR=1% [Devel: {far_01_value*100:.2f} % (threshold {far_01_threshold:.2f})]\")\n",
    "print(f\"AUXILIARY FRR @¬†FAR=5% [Devel: {ffr_05_value*100:.2f} % (threshold {frr_05_threshold:.2f})]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metrics needs to select a working point in one subset (e.g `DEVEL`) and calculate the metric in another (e.g `TEST`). For example, the `HTER` (*Half Total Error Rate*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradgpad.foundations.metrics.hter import hter\n",
    "\n",
    "\n",
    "test_scores = scores_auxiliary_test.get_numpy_scores()\n",
    "test_labels = scores_auxiliary_test.get_numpy_labels()\n",
    "\n",
    "hter_value = hter(test_scores, test_labels, eer_threshold)\n",
    "\n",
    "print(f\"HTER: {hter_value*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `BPCER`(*Bonafide Presentation Classifier Error Rate*), `APCER` (*Attack Presentation Classification Error Rate*) and `ACER` (*Average Classification Error Rate*) we have the following functions available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradgpad.foundations.metrics.apcer import apcer\n",
    "from gradgpad.foundations.metrics.bpcer import bpcer\n",
    "from gradgpad.foundations.metrics.acer import acer\n",
    "\n",
    "apcer_value = apcer(test_scores, test_labels, eer_threshold)\n",
    "bpcer_value = bpcer(test_scores, test_labels, eer_threshold)\n",
    "acer_value = acer(test_scores, test_labels, eer_threshold)\n",
    "\n",
    "print(f\"APCER: {apcer_value*100:.2f} %\")\n",
    "print(f\"BPCER: {bpcer_value*100:.2f} %\")\n",
    "print(f\"ACER: {acer_value*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gradgpad` package also has available function to obtain both `APCER` and `BPCER` fixing the opposite. In this way, we can evaluate the security of the system for an specific usability (e.g $APCER @ BPCER$) or viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradgpad.foundations.metrics.apcer_fixing_bpcer import apcer_fixing_bpcer\n",
    "\n",
    "bpcer_10 = 0.1\n",
    "apcer_value_bpcer_10 = apcer_fixing_bpcer(test_scores, test_labels, bpcer_10)\n",
    "bpcer_20 = 0.2\n",
    "apcer_value_bpcer_20 = apcer_fixing_bpcer(test_scores, test_labels, bpcer_20)\n",
    "\n",
    "print(f\"APCER @ BPCER=10%: {apcer_value_bpcer_10*100:.2f} %\")\n",
    "print(f\"APCER @ BPCER=20%: {apcer_value_bpcer_20*100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if we fix the security point, we can evaluate the usability ($APCER @ BPCER$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradgpad.foundations.metrics.bpcer_fixing_apcer import bpcer_fixing_apcer\n",
    "\n",
    "apcer_10 = 0.1\n",
    "bpcer_value_bpcer_10 = bpcer_fixing_apcer(test_scores, test_labels, apcer_10)\n",
    "apcer_20 = 0.2\n",
    "bpcer_value_bpcer_20 = bpcer_fixing_apcer(test_scores, test_labels, apcer_20)\n",
    "\n",
    "print(f\"BPCER @ APCER=10%: {bpcer_value_bpcer_10*100:.2f} %\")\n",
    "print(f\"BPCER @ APCER=20%: {bpcer_value_bpcer_20*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Results\n",
    "\n",
    "`ResultsProvider` is implemented on top on scores and provides a high level of abstraction to obtain results from Scores and Metric calculators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auxiliary_grandtest = ResultsProvider.grandtest(Approach.AUXILIARY)\n",
    "results_auxiliary_cross_dataset = ResultsProvider.cross_dataset(Approach.AUXILIARY)\n",
    "results_auxiliary_lodo = ResultsProvider.lodo(Approach.AUXILIARY)\n",
    "results_auxiliary_cross_device = ResultsProvider.cross_device(Approach.AUXILIARY)\n",
    "results_auxiliary_unseen_attack = ResultsProvider.unseen_attack(Approach.AUXILIARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the result dictionary, you can check detailed result report. This function loads the scores and calculate several working points for default face-PAD metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(results_auxiliary_grandtest, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also can use `ResultsProvider.all` to calculate each available protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auxiliary = ResultsProvider.all(Approach.AUXILIARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, `ResultsProvider` can calculate results for evenly demographic distributed subsets for the Grandtest Protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auxiliary_grandtest_sex = ResultsProvider.grandtest_fair_demographic_bpcer(\n",
    "    Approach.AUXILIARY, Demographic.SEX\n",
    ")\n",
    "results_auxiliary_grandtest_age = ResultsProvider.grandtest_fair_demographic_bpcer(\n",
    "    Approach.AUXILIARY, Demographic.AGE\n",
    ")\n",
    "results_auxiliary_grandtest_skin_tone = ResultsProvider.grandtest_fair_demographic_bpcer(\n",
    "    Approach.AUXILIARY, Demographic.SKIN_TONE\n",
    ")\n",
    "\n",
    "print(json.dumps(results_auxiliary_grandtest_sex, indent=4, sort_keys=True))\n",
    "print(json.dumps(results_auxiliary_grandtest_age, indent=4, sort_keys=True))\n",
    "print(json.dumps(results_auxiliary_grandtest_skin_tone, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
